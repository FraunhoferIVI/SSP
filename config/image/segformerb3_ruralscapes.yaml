# Folder where the checkpoint will be saved
save_dir: ./checkpoints
# Weights (of the same architecture) to load before training, leave null if no pre-training. Checkpoint should be in save_dir
pretrained_checkpoint: null
# Resume the training of a checkpoint in save_dir. The rest of this config will be discarded.
resume_training: null
# Untrained checkpoint
no_training: false

data_cfg:
  # Identifying name of the dataset, see data.datasets_utils.parse_datasets
  dataset: ruralscapes
  # Folder containing the dataset. This should be input in the dataset config in data.datasets, as DATASET.path, 
  # and left as null in this config unless the default needs to be overwritten
  path: null
  # Index of the adjacent frames, relative to the sample frame, which will be loaded alongside it. 
  # Should be -1 for the video model unless modyfing the method
  crop_size:
  - 736
  - 1280
  # Random square cropping during training to save memory (only used for the teacher model)
  square_crop: false
  # Input size, as [Height, Width]
  data_augmentation: true
  # Indicate if the labels are a probabilty distribution instead of class labels. For knowledge distillation.
  soft_labels: false
  # Enables knowledge distillation for the dataset. See data.image.image_dataset.ImageLogitsDataset
  logit_distillation: false
  # Location of the logits from the teacher model for knowledge distillation. Should point to the train_logits folder of the teacher modelÂ´s results
  logits_folder: null 
  # Min number of frames for a video to be counted. Should be left to 0 for all datasets, was used for AeroScapes only.
  min_vid_len: 0
  # Number of labeled frames used for training per video. Should use train_skip_frames instead.
  labeled_frames_per_vid: null
  # Number of frames to skip between training samples. Should only be used for knowledge distillation or when the dataset is fully labeled.
  # For knowledge distillation, set to 2. Leave to null for normal training.
  train_skip_frames: null
  # Number of frames to skip during validation step. Only useful for fully labeled datasets. Leave to 1 for others.
  val_skip_frames: 1

# See video config for details
training_cfg: 
  batch_size: 4
  num_epochs: 100
  early_stopping: 75
  num_workers: 4
  lr: 2.5e-05

# See video config for details
optim_cfg:
  optimizer: adamw
  scheduler: cosine
  weight_decay: 0.05
  # Optimizer
  momentum: 0.9
  # Scheduler
  warmup_epochs: 2
  scheduler_power: 0.9
  start_lr: 0.0001
  final_lr: 0.0001

# See video config for details
loss_cfg:
  loss_func: crossentropy
  class_weights: null
  softmax_on_target: false 
  temperature: 1

model_cfg:
  ## Model configuration
  # model checkpoint to use:
  #     Segformer: "nvidia/mit-b3" or "nvidia/segformer-b3-finetuned-cityscapes-1024-1024" or "nvidia/segformer-b2-finetuned-cityscapes-1024-1024"
  #     UPerNet: "openmmlab/upernet-swin-small" or "openmmlab/upernet-convnext-small"
  #     Hiera-sam: "tiny", "small", "base_plus"
  checkpoint_model: "nvidia/mit-b2"

  # Model architecture to use: "Segformer"/"UPerNet"/hierasam_upernet"
  # Set it as per the model checkpoint used
  model_architecture: "Segformer"
  # Never used
  frozen_encoder: false
  # Interpolation method for the upsampling. Should be consistent with the image model config. No real change on results (bilinear or bicubic).
  upsampling: "bicubic"